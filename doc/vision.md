# Техническое видение проекта

## Технологии

**Основные:**
- Python 3.11+ (современная версия с хорошей производительностью)
- uv (менеджер зависимостей и окружения)
- aiogram 3.x (современная версия для Telegram бота)
- openai (клиент для работы с OpenRouter)

**Инфраструктура:**
- Docker (контейнеризация)
- Make (автоматизация задач)

**Дополнительные:**
- python-dotenv (конфигурация)
- structlog (структурированное логирование)
- pydantic (валидация данных)

## Принцип разработки

**KISS (Keep It Simple, Stupid):**
- Минимальный функционал для проверки идеи
- Простые решения вместо сложных архитектур
- Отсутствие оверинжиниринга

**Архитектурные принципы:**
- Функциональный подход (только функции, без ООП)
- Данные в простых структурах (dict/list) в памяти
- Один файл = одна ответственность

**Код:**
- Docstring для каждой функции
- Максимум 3 уровня вложенности функций
- TDD (сначала тест, потом код)
- Рефакторинг после каждого рабочего этапа

## Структура проекта

```
llmstart-homework/
├── bot/
│   └── handlers.py    # Обработчики Telegram бота
├── llm/
│   └── client.py      # Клиент для работы с LLM
├── doc/               # Документация
│   ├── product_idea.md
│   └── vision.md
├── config.py          # Конфигурация
├── main.py            # Точка входа
├── tests/             # Тесты
├── .env.example       # Пример конфига
├── pyproject.toml     # Зависимости
├── Makefile           # Автоматизация
├── Dockerfile         # Docker образ
└── README.md
```

**Логика разделения:**
- `bot/handlers.py` - обработчики Telegram бота
- `llm/client.py` - клиент для работы с OpenRouter
- `config.py` - конфигурация приложения
- `main.py` - точка входа и инициализация
- `tests/` - все тесты

## Архитектура проекта

**Поток данных:**
1. Telegram бот получает сообщение
2. Обработчик в `bot/handlers.py` обрабатывает сообщение
3. Вызывает клиент LLM из `llm/client.py`
4. Получает ответ от OpenRouter
5. Отправляет ответ пользователю

**Взаимодействие модулей:**
- `main.py` → инициализирует бота и LLM клиент
- `bot/handlers.py` → использует `llm/client.py`
- `config.py` → используется всеми модулями
- Все модули независимы друг от друга

**Принципы:**
- Минимальные зависимости между модулями
- Четкое разделение ответственности
- Простая передача данных через функции

## Модель данных

**Основные структуры:**
```python
# История диалога пользователя
user_history = {
    "chat_id": int,
    "messages": [
        {"role": "system", "content": str},
        {"role": "user", "content": str},
        {"role": "assistant", "content": str}
    ]
}

# Конфигурация LLM
llm_config = {
    "model": str,           # модель OpenRouter
    "temperature": float,    # температура генерации
    "max_tokens": int       # максимальное количество токенов
}

# Конфигурация бота
bot_config = {
    "token": str,           # токен Telegram бота
    "webhook_url": str      # URL для webhook (опционально)
}
```

**Принципы:**
- Простые dict/list структуры
- Все данные в памяти
- Конфигурация загружается из .env в config.py
- Другие модули получают конфигурацию из config.py
- Минимальная валидация через pydantic
- Без сложных связей между данными

## Работа с LLM

**Основные функции в `llm/client.py`:**
```python
def create_client(api_key: str, base_url: str) -> OpenAI
def generate_response(client: OpenAI, messages: list, config: dict) -> str
def format_messages(history: list, user_message: str) -> list
```

**Поток работы:**
1. Инициализация OpenAI клиента с OpenRouter URL
2. Форматирование истории диалога в формат OpenAI
3. Отправка запроса с конфигурацией (модель, температура, токены)
4. Получение и возврат ответа

**Конфигурация LLM:**
- Модель: `anthropic/claude-3.5-sonnet` (через OpenRouter)
- Температура: 0.7 (баланс креативности и точности)
- Максимум токенов: 1000 (для экономии)

**Обработка ошибок:**
- Таймаут запросов: 30 секунд
- При ошибках - отправка сообщения пользователю об ошибке
- Логирование всех запросов и ответов

## Мониторинг LLM

**Логируемые данные:**
- Время запроса и ответа
- Количество токенов (входных и выходных)
- Использованная модель
- Статус запроса (успех/ошибка)
- chat_id пользователя

**Структура лога:**
```python
{
    "timestamp": "2024-01-01T12:00:00Z",
    "chat_id": 123456,
    "model": "anthropic/claude-3.5-sonnet",
    "tokens_in": 150,
    "tokens_out": 50,
    "response_time_ms": 2500,
    "status": "success"
}
```

**Принципы:**
- Структурированное логирование через structlog
- Логирование всех данных включая содержимое сообщений
- Простой анализ через grep/awk

## Сценарии работы

**1. Старт бота:**
- Пользователь отправляет `/start`
- Бот приветствует и объясняет возможности
- Создается новая история диалога

**2. Обычный диалог:**
- Пользователь отправляет сообщение
- Бот добавляет сообщение в историю
- Отправляет запрос к LLM
- Получает ответ и отправляет пользователю
- Добавляет ответ в историю

**3. Обработка ошибок:**
- При ошибке LLM - отправка "Извините, не удалось обработать запрос"
- Реальная ошибка логируется для анализа
- При сетевых проблемах - уведомление пользователя

**4. Ограничения:**
- Максимум 5 сообщений в истории (экономия токенов)
- При превышении - удаление старых сообщений
- Таймаут 30 секунд на запрос

## Деплой

**Docker контейнер:**
- Один контейнер с Python приложением
- Мульти-стадия сборка для оптимизации размера
- Использование uv для установки зависимостей


**Make команды:**
```bash
make build    # сборка Docker образа
make run      # запуск в Docker
make test     # запуск тестов
make logs     # просмотр логов
```

**Принципы:**
- Один образ для всех окружений
- Конфигурация через переменные окружения
- Простой процесс развертывания

## Подход к конфигурированию

**Структура .env файла:**
```bash
# Telegram Bot
TELEGRAM_BOT_TOKEN=your_bot_token

# OpenRouter
OPENROUTER_API_KEY=your_api_key
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# LLM Configuration
LLM_MODEL=anthropic/claude-3.5-sonnet
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1000
LLM_SYSTEM_PROMPT=You are a helpful assistant.

# Logging
LOG_LEVEL=INFO
```

**config.py функции:**
```python
def load_config() -> dict
def get_bot_config() -> dict
def get_llm_config() -> dict
def validate_config(config: dict) -> bool
```

**Принципы:**
- Все настройки в .env файле
- Валидация конфигурации при старте
- Значения по умолчанию для разработки
- Без хардкода в коде
- Использование поллинга для Telegram бота

## Подход к логгированию

**Структурированное логирование:**
```python
import structlog

logger = structlog.get_logger()
logger.info("user_message", chat_id=123, message_length=50)
logger.error("llm_error", chat_id=123, error_type="timeout")
```

**Уровни логирования:**
- INFO: обычные операции (запросы, ответы)
- ERROR: ошибки и исключения
- DEBUG: детальная отладочная информация

**Логируемые события:**
- Старт/остановка бота
- Получение сообщений от пользователей
- Запросы к LLM и ответы
- Ошибки и исключения
- Статистика использования

**Принципы:**
- Структурированные JSON логи
- Логирование всех данных включая содержимое сообщений
- Простота анализа через grep/awk
- Ротация логов для экономии места
